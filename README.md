# ai-unit-test-generator

> AI-powered unit test generator with intelligent priority scoring

[![npm version](https://img.shields.io/npm/v/ai-unit-test-generator.svg)](https://www.npmjs.com/package/ai-unit-test-generator)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## ğŸ¯ Key Features

### Core Features
- **ğŸ—ï¸ Layered Architecture Scoring**: Intelligent scoring based on code layers (Foundation, Business, State, UI)
- **ğŸ¤– AI-Native Design**: Perfect integration with Cursor Agent, ChatGPT, Claude, and more
- **ğŸ“Š Coverage-Aware**: Integrates code coverage data for smart prioritization (incremental & existing code)
- **ğŸ¨ Multi-Dimensional Scoring**: Combines testability, complexity, dependency count, business criticality, error risk
- **âš¡ Batch Generation**: Automated batch test generation with failure retry and progress tracking
- **ğŸ“ Rich Reports**: Generates detailed scoring reports in Markdown and CSV formats
- **ğŸ”„ Status Management**: Automatic progress tracking (TODO/DONE/SKIP)

### Advanced Features (v2.1+)
- **ğŸ”„ Iterative Improvement**: Meta TestGen-LLM style auto-improvement until quality standards met
- **ğŸ² N-Sample Generation**: Generate multiple candidates, select best (Meta Section 4.2)
- **ğŸ” Safety Features**: File write protection, automatic backups, dry-run mode
- **âœ… Stability Checks**: Run tests multiple times to ensure consistency

### Competitive Features (v2.3.0 - Keploy & Qodo Inspired)
- **ğŸ¯ Boundary Detection**: Automatic identification of parameter and condition boundaries
- **ğŸ“Š Cobertura Coverage**: Line-level precision coverage analysis (Keploy required format)
- **ğŸ”Œ Mock Analysis**: Intelligent dependency detection with recommended mock strategies

### Phase 2 Features (v2.4.0)
- **ğŸš€ Parallel Generation**: Multi-threaded test generation for 2-3x speed improvement using p-limit
- **ğŸ­ Behavior Classification**: Automatic categorization into Happy Path, Edge Case, and Error Path tests (Qodo Cover style)

## ğŸ“¦ Installation

### Global Installation (Recommended)

```bash
npm install -g ai-unit-test-generator
```

### Project Installation

```bash
npm install --save-dev ai-unit-test-generator
```

## ğŸš€ Quick Start

### 1. Scan Code and Generate Priority Report

```bash
ai-test scan
```

On first run, it automatically generates a config file `ai-test.config.jsonc` with detailed comments.

The scan will:
1. Analyze code structure and identify testable targets
2. Auto-run Jest coverage analysis (if installed)
3. Analyze Git history for error signals
4. Calculate multi-dimensional priority scores
5. Generate sorted report (`reports/ut_scores.md`)

**Output Example:**

```markdown
| Status | Score | Priority | Name | Type | Layer | Path | Coverage | CS | BC | CC | ER | Testability | DepCount |
|--------|-------|----------|------|------|-------|------|----------|----|----|----|----|-------------|----------|
| TODO | 8.5 | P0 | validatePayment | function | Business Logic | src/services/payment.ts | 0.0% | 10 | 10 | 8 | 7 | 9 | 12 |
| TODO | 7.8 | P0 | formatCurrency | function | Foundation | src/utils/format.ts | 15.0% | 8 | 6 | 5 | 8 | 10 | 8 |
```

### 2. Generate Tests

```bash
# Generate tests for 10 functions (with auto-improvement, default)
ai-test generate

# Generate tests for 20 functions (with auto-improvement)
ai-test generate -n 20

# ğŸ² N-Sample Generation: Generate 3 candidates per iteration, pick best
ai-test generate --samples 3

# Generate without auto-improvement (one-shot mode)
ai-test generate --no-iterative
```

The command will:
1. Select the top N highest-priority untested functions from the report
2. Invoke Cursor Agent to auto-generate tests
3. Extract and save test files
4. Run Jest to validate tests
5. **ğŸ”„ Check quality and auto-improve if needed (DEFAULT in v2.1+)**
6. Auto-update report status to DONE

**ğŸ”„ Iterative Mode** (DEFAULT in v2.1):
- **Enabled by default** - automatically improves test quality
- Checks: Build success (75%), test pass rate (57%), coverage increase (25%)
- Re-generates with feedback until quality standards met or max iterations (3)
- Based on Meta's TestGen-LLM research (2024)
- Use `--no-iterative` to disable (one-shot mode)

**ğŸ² N-Sample Generation** (NEW in v2.1.2):
- **Advanced mode** - generate multiple candidates, select best
- Use `--samples 3` to generate 3 candidates per iteration
- Scoring: Build (40%) + Tests Pass (30%) + Coverage (30%)
- Trade-off: Higher success rate vs longer time
- Recommended for critical/complex functions

### 3. Enable Cobertura Coverage (Recommended for v2.3+)

For **line-level precision** coverage (Keploy style), enable Cobertura format:

```bash
# Copy template
cp node_modules/ai-unit-test-generator/templates/jest.config.cobertura.js jest.config.js

# Or add to existing jest.config.js:
module.exports = {
  coverageReporters: ['text', 'cobertura', 'lcov'],  // â­ Add cobertura
  // ... other config
}
```

Then run coverage:
```bash
npm test -- --coverage
```

**Why Cobertura?**
- âœ… Line-level precision (vs statement-level)
- âœ… Better AI guidance for uncovered lines
- âœ… Industry standard (Keploy, SonarQube, Jenkins)

ğŸ“– **Full Guide**: [docs/COBERTURA_SETUP.md](docs/COBERTURA_SETUP.md)

### 4. Track Progress

```bash
# View report
cat reports/ut_scores.md

# View P0 priority tasks
grep "| P0 |" reports/ut_scores.md

# View pending tasks
grep "| TODO |" reports/ut_scores.md
```

## ğŸ“– CLI Commands

### `ai-test scan`

Scan code and generate priority report

```bash
ai-test scan [options]

Options:
  -c, --config <path>   Config file path (default: ai-test.config.jsonc)
  -o, --output <dir>    Output directory (default: reports)
  --skip-git            Skip Git history analysis
```

**Output Files:**
- `reports/targets.json` - List of scanned targets
- `reports/git_signals.json` - Git analysis data
- `reports/ut_scores.md` - Markdown format report (sorted by score)
- `reports/ut_scores.csv` - CSV format report

### `ai-test generate`

Generate unit tests (using Cursor Agent)

```bash
ai-test generate [options]

Options:
  -n, --count <number>           Number of functions to generate tests for (default: 10)
  -p, --priority <level>         Priority filter (P0, P1, P2, P3)
  --all                          Generate all remaining TODO functions
  --no-iterative                 Disable iterative improvement (default: enabled)
  --max-iterations <number>      Maximum iterations for iterative mode (default: 3)
  --report <path>                Report file path (default: reports/ut_scores.md)
```

**Examples:**
```bash
ai-test generate                 # Generate top 10 with auto-improvement (default)
ai-test generate -n 20           # Generate top 20 with auto-improvement
ai-test generate -p P0           # Only P0 functions (if any)
ai-test generate --all           # Generate all TODO functions
ai-test generate --no-iterative  # Disable auto-improvement (one-shot mode)
```

The command automatically:
- Selects highest-priority untested functions
- Invokes Cursor Agent for test generation
- Extracts and saves test files
- Runs Jest validation
- Auto-marks DONE on success

### `ai-test parallel` (NEW in v2.4.0 ğŸš€)

**Parallel test generation** - 2-3x faster than sequential mode

```bash
ai-test parallel [options]

Options:
  -n, --count <number>        Number of functions to generate (default: all TODO)
  -p, --priority <level>      Priority filter (P0, P1, P2, P3)
  -c, --concurrency <number>  Concurrent batches (default: 3, max: 5)
  --report <path>             Report file path (default: reports/ut_scores.md)
```

**Examples:**
```bash
ai-test parallel                   # All TODO functions, 3 concurrent batches
ai-test parallel -n 30             # Top 30 functions, 3 concurrent
ai-test parallel -p P0 -c 5        # All P0 functions, 5 concurrent
ai-test parallel -n 50 -c 4        # Top 50 functions, 4 concurrent
```

**How it works:**
1. Groups functions by file (better context efficiency)
2. Creates balanced batches (3-10 functions each)
3. Runs batches concurrently with p-limit control
4. Each batch: Prompt â†’ AI â†’ Extract â†’ Test (independent)
5. Merges results and auto-updates report

**Performance:**
- âš¡ **2-3x faster** than `ai-test generate`
- ğŸ¯ **Smart batching** by file grouping
- ğŸ” **API-safe** with max concurrency limits
- ğŸ“Š **Detailed telemetry** in `reports/parallel_batches/parallel_report.json`

**When to use:**
- âœ… Large codebases (30+ TODO functions)
- âœ… Need faster turnaround
- âœ… Stable API quota
- âŒ Don't use with iterative mode (use `generate` instead)

**Reference:**
- Qodo Cover: Parallel generation strategy
- AutoTestGen: Batch processing optimization

### Other Commands

**ğŸ”„ Iterative Improvement Mode** (DEFAULT):
- **Enabled by default** - no flag needed
- **Quality Standards** (from Meta TestGen-LLM):
  - 75% build success (TypeScript compilation)
  - 57% test pass rate
  - 25% coverage increase
- **Process**:
  1. Generate tests
  2. Check quality (build, tests, coverage)
  3. If not met â†’ Collect feedback â†’ Regenerate
  4. Repeat until: Quality met OR Max iterations (default: 3)
- **Output**: `reports/improvement_report.json`
- **To disable**: Use `--no-iterative` flag

## âš™ï¸ Configuration

On first run of `ai-test scan`, a config file `ai-test.config.jsonc` is auto-generated with detailed comments.

### Core Configuration

```jsonc
{
  "scoringMode": "layered",  // Scoring mode: layered or legacy
  
  // Coverage configuration
  "coverage": {
    "runBeforeScan": true,  // Auto-run Jest coverage before scan
    "command": "npx jest --coverage --silent",
    "summaryPath": "coverage/coverage-summary.json"
  },
  
  // Coverage scoring mapping
  "coverageScoring": {
    "naScore": 5,  // Default score when coverage data is unavailable
    "mapping": [
      { "lte": 0, "score": 10 },    // 0% coverage â†’ highest priority
      { "lte": 40, "score": 8 },    // 1-40% â†’ high priority
      { "lte": 70, "score": 6 },    // 41-70% â†’ medium priority
      { "lte": 90, "score": 3 },    // 71-90% â†’ low priority
      { "lte": 100, "score": 1 }    // 91-100% â†’ lowest priority
    ]
  },
  
  // Layer configuration
  "layers": {
    "foundation": {
      "name": "Foundation (Utils & Helpers)",
      "patterns": ["**/utils/**", "**/helpers/**", "**/constants/**"],
      "weights": {
        "testability": 0.30,      // Testability weight
        "dependencyCount": 0.25,  // Dependency count weight
        "complexity": 0.15,       // Complexity weight
        "BC": 0.10,              // Business criticality weight
        "ER": 0.10,              // Error risk weight
        "coverage": 0.10          // Coverage weight
      },
      "thresholds": {
        "P0": 8.0,  // Score â‰¥8.0 = P0 (must test)
        "P1": 6.5,  // Score 6.5-7.9 = P1 (high priority)
        "P2": 5.0   // Score 5.0-6.4 = P2 (medium priority), <5.0 = P3
      }
    }
    // ... other layer configurations
  }
}
```

## ğŸ“Š Priority Levels

| Priority | Score Range | Description | Action |
|----------|-------------|-------------|--------|
| **P0** | â‰¥8.0 | Must test | Generate immediately |
| **P1** | 6.5-7.9 | High priority | Batch generate |
| **P2** | 5.0-6.4 | Medium priority | Generate with review |
| **P3** | <5.0 | Low priority | Optional coverage |

## ğŸ—ï¸ Layered Architecture

### 1. Foundation Layer
- **Characteristics**: Utility functions, helpers, constants
- **Weights**: High testability weight (30%)
- **Threshold**: P0 â‰¥ 8.0

### 2. Business Logic Layer
- **Characteristics**: Services, APIs, data processing
- **Weights**: Balanced multi-dimensional scoring
- **Threshold**: P0 â‰¥ 7.5

### 3. State Management Layer
- **Characteristics**: State stores, contexts, reducers
- **Weights**: Emphasizes error risk
- **Threshold**: P0 â‰¥ 7.0

### 4. UI Components Layer
- **Characteristics**: React components, views
- **Weights**: Balanced complexity and error risk
- **Threshold**: P0 â‰¥ 6.5

## ğŸ“ˆ Scoring Metrics Explained

### Coverage Score (CS)

**New Feature**: Integrates code coverage data for both incremental and existing code scenarios.

- **Score Mapping**:
  - 0% â†’ 10 points (highest priority, needs testing urgently)
  - 1-40% â†’ 8 points (high priority)
  - 41-70% â†’ 6 points (medium priority)
  - 71-90% â†’ 3 points (low priority)
  - 91-100% â†’ 1 point (well covered)
  - N/A â†’ 5 points (no data available)

- **Weight**: Configured per layer (typically 10-20%)

### Testability

- **Pure Functions**: 10/10 (no side effects, easy to test)
- **Simple Mocks**: 8-9/10 (dependencies easy to mock)
- **Complex Dependencies**: 4-6/10 (requires complex test setup)

### Dependency Count

Based on reference count:
- **â‰¥10 modules referencing**: 10/10 (core module)
- **5-9 modules**: 10/10
- **3-4 modules**: 9/10
- **1-2 modules**: 7/10
- **No references**: 5/10

### Complexity

- **Cyclomatic Complexity**: 11-15 â†’ 10/10 (medium complexity, worth testing)
- **Cognitive Complexity**: Analyzed via ESLint
- **Nesting Depth**: Adjusts complexity score

### Business Criticality (BC)

Based on Git history:
- Modification frequency
- Number of contributors
- Commit message keywords (fix, bug, hotfix)

### Error Risk (ER)

Based on:
- Error handling code
- Try-catch blocks
- Number of conditional branches

## ğŸ¤– AI Integration

### Using Cursor Agent (Recommended)

```bash
# Auto-generate tests (built-in Cursor Agent integration)
ai-test generate -n 10
```

### Using Other AI Tools (ChatGPT, Claude)

```bash
# 1. Generate AI prompt
ai-test scan
grep "| TODO |" reports/ut_scores.md | head -10

# 2. Manually copy function info to AI tool
# 3. Save AI response to a file
# 4. Run tests for validation
npm test
```

## ğŸ¬ Complete Workflow Example

```bash
# 1. Install Jest (if not already installed)
npm i -D jest@29 ts-jest@29 @types/jest@29 jest-environment-jsdom@29

# 2. Scan code
ai-test scan
# âœ… Auto-generates config file
# âœ… Auto-runs coverage analysis
# âœ… Generates priority report

# 3. View report
cat reports/ut_scores.md

# 4. Generate tests (10 functions)
ai-test generate

# 5. View results
npm test

# 6. Continue with next batch
ai-test generate -n 10

# 7. Repeat until all high-priority tests are complete
```

## ğŸ› ï¸ Advanced Usage

### Jest Environment Requirements

First-time users need to install Jest:

```bash
npm i -D jest@29 ts-jest@29 @types/jest@29 jest-environment-jsdom@29
```

Then add type support in `tsconfig.json`:

```json
{
  "compilerOptions": {
    "typeRoots": ["node_modules/@types"]
  }
}
```

### Custom Coverage Command

Modify `ai-test.config.jsonc`:

```jsonc
{
  "coverage": {
    "runBeforeScan": true,
    "command": "npm run test:coverage"  // Custom command
  }
}
```

### Skip Git Analysis

If project has no Git history or Git signals are not needed:

```bash
ai-test scan --skip-git
```

## ğŸ“š Inspiration

This project draws inspiration from research and practices:

- **Meta TestGen-LLM**: Quality assurance filters and at-scale practices  
  [Automated Unit Test Improvement using Large Language Models at Meta](https://arxiv.org/abs/2402.09171)

- **ByteDance Midscene.js**: Natural language interface and stability practices  
  https://github.com/web-infra-dev/midscene

- **Airbnb**: Large-scale LLM-assisted migration and batching  
  https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b

- **TestART**: Iterative generation and template repair  
  https://arxiv.org/abs/2408.03095

These ideas are reflected in `ai-unit-test-generator` as:
- Strict output protocol (JSON manifest + code blocks)
- Failure feedback loop (Jest JSON â†’ actionable hints â†’ next prompt)
- Batch processing with progress tracking (TODO/DONE/SKIP)
- Coverage-aware prioritization

## ğŸ”§ Project Structure

```
ai-unit-test-generator/
â”œâ”€â”€ bin/
â”‚   â””â”€â”€ cli.js                    # CLI entry point
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ core/                     # æ ¸å¿ƒåˆ†æå±‚ï¼ˆæ—  AI ä¾èµ–ï¼‰
â”‚   â”‚   â”œâ”€â”€ scanner.mjs          # AST æ‰«æ + å…ƒæ•°æ®æå–
â”‚   â”‚   â”œâ”€â”€ git-analyzer.mjs     # Git å†å²åˆ†æ
â”‚   â”‚   â”œâ”€â”€ scorer.mjs           # è¯„åˆ†å¼•æ“ + AI å¢å¼º
â”‚   â”‚   â””â”€â”€ index.mjs            # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ ai/                       # AI äº¤äº’å±‚
â”‚   â”‚   â”œâ”€â”€ sampler.mjs          # æ™ºèƒ½ä»£ç é‡‡æ ·
â”‚   â”‚   â”œâ”€â”€ context-builder.mjs  # é¡¹ç›®ä¸Šä¸‹æ–‡æ„å»º
â”‚   â”‚   â”œâ”€â”€ analyzer-prompt.mjs  # AI åˆ†æ Prompt
â”‚   â”‚   â”œâ”€â”€ validator.mjs        # å“åº”éªŒè¯
â”‚   â”‚   â”œâ”€â”€ reviewer.mjs         # äº¤äº’å¼å®¡æ ¸ UI
â”‚   â”‚   â”œâ”€â”€ config-writer.mjs    # å®‰å…¨é…ç½®å†™å…¥
â”‚   â”‚   â”œâ”€â”€ prompt-builder.mjs   # æµ‹è¯•ç”Ÿæˆ Prompt
â”‚   â”‚   â”œâ”€â”€ client.mjs           # Cursor Agent è°ƒç”¨
â”‚   â”‚   â”œâ”€â”€ extractor.mjs        # æµ‹è¯•æå–
â”‚   â”‚   â””â”€â”€ index.mjs            # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ testing/                  # æµ‹è¯•æ‰§è¡Œå±‚
â”‚   â”‚   â”œâ”€â”€ runner.mjs           # Jest è¿è¡Œå™¨
â”‚   â”‚   â”œâ”€â”€ analyzer.mjs         # å¤±è´¥åˆ†æ
â”‚   â”‚   â”œâ”€â”€ coverage-runner.mjs  # è¦†ç›–ç‡åˆ†æ
â”‚   â”‚   â””â”€â”€ index.mjs            # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ workflows/                # å·¥ä½œæµç¼–æ’å±‚
â”‚   â”‚   â”œâ”€â”€ init.mjs             # åˆå§‹åŒ–å·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ analyze.mjs          # AI åˆ†æå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ scan.mjs             # æ‰«æå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ generate.mjs         # ç”Ÿæˆå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ batch.mjs            # æ‰¹å¤„ç†
â”‚   â”‚   â”œâ”€â”€ all.mjs              # å…¨è‡ªåŠ¨
â”‚   â”‚   â””â”€â”€ index.mjs            # æ¨¡å—å¯¼å‡º
â”‚   â”œâ”€â”€ utils/                    # å·¥å…·å±‚
â”‚   â”‚   â”œâ”€â”€ config-manager.mjs   # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ scan-manager.mjs     # æ‰«æç®¡ç†
â”‚   â”‚   â”œâ”€â”€ marker.mjs           # çŠ¶æ€æ ‡è®°
â”‚   â”‚   â””â”€â”€ index.mjs            # æ¨¡å—å¯¼å‡º
â”‚   â””â”€â”€ index.mjs                 # æ ¹å¯¼å‡º
â””â”€â”€ templates/
    â””â”€â”€ default.config.jsonc      # é»˜è®¤é…ç½®æ¨¡æ¿
```

### Architecture Principles

- **Layered Design**: Clear separation between core analysis, AI interaction, testing, and workflows
- **Zero AI Dependency in Core**: Core modules can be used without AI features
- **Modular Exports**: Each layer has clean API exports
- **Programmatic API**: All workflows can be imported and used programmatically

## ğŸ—ï¸ Project Architecture (v3.0+)

### TypeScript Migration

Starting from v3.0.0, the project has been fully migrated to TypeScript:

```
src/
â”œâ”€â”€ shared/          âœ¨ Shared utilities layer (NEW in v3.0.1)
â”‚   â”œâ”€â”€ cli-utils.ts    - CLI parsing, messages
â”‚   â”œâ”€â”€ file-utils.ts   - File I/O, JSONC support
â”‚   â”œâ”€â”€ process-utils.ts - Commands, packages
â”‚   â””â”€â”€ path-utils.ts   - Paths, glob matching
â”œâ”€â”€ types/           ğŸ“˜ Type definitions (5 modules)
â”‚   â”œâ”€â”€ index.ts        - Core types
â”‚   â”œâ”€â”€ cli.ts          - CLI options
â”‚   â”œâ”€â”€ coverage.ts     - Coverage data
â”‚   â”œâ”€â”€ quality.ts      - Quality metrics
â”‚   â””â”€â”€ parallel.ts     - Parallel config
â”œâ”€â”€ core/            ğŸ§  Analysis engine
â”œâ”€â”€ ai/              ğŸ¤– AI interaction
â”œâ”€â”€ testing/         âœ… Test execution
â”œâ”€â”€ workflows/       ğŸ”„ Orchestration
â””â”€â”€ cli.ts           ğŸ¯ CLI entry
```

### Key Improvements

- **Type Coverage**: 39% fully typed (growing)
- **Code Quality**: 85%+ duplicate code eliminated
- **Architecture**: Shared utilities layer for consistency
- **Documentation**: See [TYPESCRIPT_MIGRATION.md](./TYPESCRIPT_MIGRATION.md) and [STRUCTURE_OPTIMIZATION.md](./STRUCTURE_OPTIMIZATION.md)

## ğŸ¤ Contributing

Contributions are welcome! Please submit issues or pull requests.

## ğŸ“„ License

MIT Â© YuhengZhou

## ğŸ”— Links

- [npm Package](https://www.npmjs.com/package/ai-unit-test-generator)
- [Technical Documentation](./tech_doc.md)
- [Changelog](./CHANGELOG.md)

## ğŸ’¬ Support

Need help? Get support through:

- Read [Technical Documentation](./tech_doc.md)
- Check [Changelog](./CHANGELOG.md)
- Submit [GitHub Issues](https://github.com/temptrip/ai-unit-test-generator/issues)

---

**Tip**: For first-time users, it's recommended to test on a small project first to familiarize yourself with the workflow before applying to larger projects.
