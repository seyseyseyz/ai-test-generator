#!/usr/bin/env node
/**
 * å¹¶è¡Œæµ‹è¯•ç”Ÿæˆï¼ˆPhase 2 - v2.4.0ï¼‰
 * 
 * ä½¿ç”¨ p-limit å®ç°å¹¶å‘ç”Ÿæˆï¼Œ2-3x é€Ÿåº¦æå‡ã€‚
 * 
 * ç­–ç•¥:
 * 1. å°† TODO å‡½æ•°åˆ†ç»„ï¼ˆæŒ‰æ–‡ä»¶æˆ–å›ºå®šå¤§å°ï¼‰
 * 2. å¹¶å‘ç”Ÿæˆå¤šä¸ªæ‰¹æ¬¡ï¼ˆæ§åˆ¶å¹¶å‘æ•°ï¼‰
 * 3. ç‹¬ç«‹çš„ Prompt â†’ AI â†’ Extract â†’ Test æµç¨‹
 * 4. æ±‡æ€»ç»“æœå¹¶æ›´æ–°æŠ¥å‘Š
 * 
 * å‚è€ƒ:
 * - Qodo Cover çš„å¹¶è¡Œç”Ÿæˆç­–ç•¥
 * - AutoTestGen çš„æ‰¹å¤„ç†ä¼˜åŒ–
 * 
 * @module parallel-generate
 */

import { spawn } from 'node:child_process'
import { existsSync, readFileSync, writeFileSync, mkdirSync } from 'node:fs'
import { fileURLToPath } from 'node:url'
import { dirname, join } from 'node:path'
import pLimit from 'p-limit'

const __filename = fileURLToPath(import.meta.url)
const __dirname = dirname(__filename)
const PKG_ROOT = join(__dirname, '../..')

// å¹¶å‘æ§åˆ¶é…ç½®
const CONCURRENCY_CONFIG = {
  default: 3,      // é»˜è®¤å¹¶å‘æ•°
  maxConcurrency: 5, // æœ€å¤§å¹¶å‘æ•°ï¼ˆé¿å… API é™æµï¼‰
  minBatchSize: 3,  // æœ€å°æ‰¹æ¬¡å¤§å°
  maxBatchSize: 10  // æœ€å¤§æ‰¹æ¬¡å¤§å°
}

/**
 * è¾…åŠ©å‡½æ•°ï¼šè¿è¡Œå­è¿›ç¨‹
 */
function sh(cmd: string, args: string[], options: any): Promise<string | null> {
  return new Promise((resolve, reject) => {
    const stdio: any = options.captureStdout ? ['inherit', 'pipe', 'inherit'] : 'inherit'
    const child: any = spawn(cmd, args, { 
      stdio, 
      cwd: options.cwd || process.cwd(),
      env: { ...process.env, ...options.env }
    })
    
    const chunks: Buffer[] = []
    if (options.captureStdout) {
      child.stdout.on('data', (d: Buffer) => chunks.push(d))
    }
    
    child.on('close', (code: number) => {
      if (code === 0) {
        const output = options.captureStdout ? Buffer.concat(chunks).toString('utf8') : null
        resolve(output)
      } else {
        reject(new Error(`${cmd} exited ${code}`))
      }
    })
    child.on('error', reject)
  })
}

/**
 * è¯»å– TODO å‡½æ•°åˆ—è¡¨
 */
function readTodoFunctions(reportPath: string, priority: string | null): any[] {
  if (!existsSync(reportPath)) {
    throw new Error(`Report not found: ${reportPath}`)
  }
  
  const content = readFileSync(reportPath, 'utf-8')
  const lines = content.split('\n')
  
  const todoFunctions = []
  for (const line of lines) {
    if (!line.includes('| TODO |')) continue
    
    if (priority && !line.includes(`| ${priority} |`)) continue
    
    const parts = line.split('|').map(p => p.trim()).filter(Boolean)
    if (parts.length >= 7) {
      const [_status, score, pri, name, type, layer, path] = parts
      todoFunctions.push({
        name: (name || '').toString(),
        path: (path || '').toString(),
        score: parseFloat(score || '0'),
        priority: (pri || '').toString(),
        type: (type || '').toString(),
        layer: (layer || '').toString()
      })
    }
  }
  
  // æŒ‰åˆ†æ•°é™åºæ’åº
  if (!priority) {
    todoFunctions.sort((a, b) => b.score - a.score)
  }
  
  return todoFunctions
}

/**
 * å°†å‡½æ•°åˆ†ç»„ä¸ºæ‰¹æ¬¡
 * 
 * ç­–ç•¥:
 * - æŒ‰æ–‡ä»¶åˆ†ç»„ï¼ˆåŒä¸€æ–‡ä»¶çš„å‡½æ•°æ”¾åœ¨ä¸€èµ·ï¼Œæé«˜ä¸Šä¸‹æ–‡æ•ˆç‡ï¼‰
 * - æ§åˆ¶æ‰¹æ¬¡å¤§å°åœ¨åˆç†èŒƒå›´å†…
 */
function groupIntoBatches(functions: any[], options: any = {}): any[] {
  const { minBatchSize, maxBatchSize } = { ...CONCURRENCY_CONFIG, ...options }
  
  // æŒ‰æ–‡ä»¶åˆ†ç»„
  const byFile: any = {}
  for (const func of functions) {
    if (!byFile[func.path]) {
      byFile[func.path] = []
    }
    byFile[func.path].push(func)
  }
  
  // è½¬æ¢ä¸ºæ‰¹æ¬¡åˆ—è¡¨
  const batches: any[] = []
  for (const filePath in byFile) {
    const fileFunctions = byFile[filePath]
    
    // å¦‚æœä¸€ä¸ªæ–‡ä»¶çš„å‡½æ•°å¤ªå¤šï¼Œæ‹†åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
    if (fileFunctions.length > maxBatchSize) {
      for (let i = 0; i < fileFunctions.length; i += maxBatchSize) {
        batches.push(fileFunctions.slice(i, i + maxBatchSize))
      }
    } else {
      batches.push(fileFunctions)
    }
  }
  
  // åˆå¹¶è¿‡å°çš„æ‰¹æ¬¡
  const finalBatches = []
  let currentBatch = []
  
  for (const batch of batches) {
    currentBatch.push(...batch)
    
    if (currentBatch.length >= minBatchSize) {
      finalBatches.push(currentBatch)
      currentBatch = []
    }
  }
  
  // å¤„ç†å‰©ä½™çš„å‡½æ•°
  if (currentBatch.length > 0) {
    if (finalBatches.length > 0 && currentBatch.length < minBatchSize) {
      // åˆå¹¶åˆ°æœ€åä¸€ä¸ªæ‰¹æ¬¡
      const lastBatch = finalBatches[finalBatches.length - 1]
      if (lastBatch) lastBatch.push(...currentBatch)
    } else {
      finalBatches.push(currentBatch)
    }
  }
  
  return finalBatches
}

/**
 * ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•
 * 
 * æ¯ä¸ªæ‰¹æ¬¡ç‹¬ç«‹è¿è¡Œï¼š
 * 1. ç”Ÿæˆ Prompt
 * 2. è°ƒç”¨ AI
 * 3. æå–æµ‹è¯•
 * 4. è¿è¡Œ Jest
 */
async function generateBatch(batch: any[], batchIndex: number, options: any = {}): Promise<any> {
  const { reportPath, workDir, config: _config } = options
  
  console.log(`\nğŸ”„ [Batch ${batchIndex + 1}] Generating ${batch.length} functions...`)
  console.log(`   Files: ${[...new Set(batch.map((f: any) => f.path))].join(', ')}`)
  
  const batchDir = join(workDir, `batch_${batchIndex}`)
  mkdirSync(batchDir, { recursive: true })
  
  const result = {
    batchIndex,
    total: batch.length,
    success: [] as string[],
    failed: [] as string[],
    error: null as any
  }
  
  try {
    // 1. ç”Ÿæˆ Promptï¼ˆåªåŒ…å«è¿™ä¸ªæ‰¹æ¬¡çš„å‡½æ•°ï¼‰
    const promptArgs = [
      join(PKG_ROOT, 'lib/ai/prompt-builder.mjs'),
      '--report', reportPath,
      '--only-todo'
    ]
    
    // å†™å…¥æ‰¹æ¬¡å‡½æ•°åˆ—è¡¨
    const batchListPath = join(batchDir, 'functions.txt')
    writeFileSync(batchListPath, batch.map(f => f.name).join('\n'), 'utf-8')
    promptArgs.push('--function-list', batchListPath)
    
    let promptText: string | null = null
    try {
      // const _hintsFile = join(batchDir, 'hints.txt')
      if (existsSync('reports/hints.txt')) {
        promptText = await sh('node', [...promptArgs, '--hints-file', 'reports/hints.txt'], { 
          captureStdout: true,
          cwd: process.cwd()
        })
      } else {
        promptText = await sh('node', promptArgs, { captureStdout: true })
      }
    } catch (err) {
      promptText = await sh('node', promptArgs, { captureStdout: true })
    }
    
    const promptPath = join(batchDir, 'prompt.txt')
    writeFileSync(promptPath, promptText || '', 'utf-8')
    
    // 2. è°ƒç”¨ AI
    const aiResponsePath = join(batchDir, 'ai_response.txt')
    await sh('node', [
      join(PKG_ROOT, 'lib/ai/client.mjs'),
      '--prompt', promptPath,
      '--out', aiResponsePath
    ], { captureStdout: false, cwd: process.cwd(), env: {} })
    
    // 3. æå–æµ‹è¯•
    await sh('node', [
      join(PKG_ROOT, 'lib/ai/extractor.mjs'),
      aiResponsePath,
      '--overwrite'
    ], { captureStdout: false, cwd: process.cwd(), env: {} })
    
    // 4. è¿è¡Œ Jestï¼ˆåªé’ˆå¯¹è¿™ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•æ–‡ä»¶ï¼‰
    const testFiles = [...new Set(batch.map((f: any) => (f.path || '').replace(/\.(ts|tsx|js|jsx)$/i, (m: string) => `.test${m}`)))]
    
    try {
      await sh('npm', ['test', '--', ...testFiles], { captureStdout: false, cwd: process.cwd(), env: {} })
      result.success = batch.map((f: any) => f.name) as string[]
      console.log(`âœ… [Batch ${batchIndex + 1}] All tests passed`)
    } catch (err: any) {
      // å³ä½¿æµ‹è¯•å¤±è´¥ä¹Ÿç»§ç»­ï¼ˆä¼šåœ¨åç»­åˆ†æä¸­å¤„ç†ï¼‰
      console.warn(`âš ï¸  [Batch ${batchIndex + 1}] Some tests failed`)
      result.failed = batch.map((f: any) => f.name) as string[]
    }
    
  } catch (err: any) {
    const error = err as Error
    console.error(`âŒ [Batch ${batchIndex + 1}] Failed:`, error?.message || String(err))
    result.error = error?.message || String(err)
    result.failed = batch.map((f: any) => f.name) as string[]
  }
  
  return result
}

/**
 * æ ‡è®°å‡½æ•°çŠ¶æ€
 */
function updateFunctionStatus(reportPath: string, functionNames: string[], status: string): void {
  if (!existsSync(reportPath)) return
  
  let content = readFileSync(reportPath, 'utf-8')
  
  for (const name of functionNames) {
    if (!name) continue
    const lines = content.split('\n')
    for (let i = 0; i < lines.length; i++) {
      const line = lines[i]
      if (line?.includes(`| ${name} |`) && line.includes('| TODO |')) {
        lines[i] = line.replace('| TODO |', `| ${status} |`)
      }
    }
    content = lines.join('\n')
  }
  
  writeFileSync(reportPath, content, 'utf-8')
}

/**
 * ä¸»å‡½æ•°ï¼šå¹¶è¡Œç”Ÿæˆæµ‹è¯•
 */
export async function parallelGenerate(options: any = {}): Promise<void> {
  const reportPath: string = (options.reportPath as string) || 'reports/ut_scores.md'
  const priority: string | null = (options.priority as string) || null
  const count: number | null = (options.count as number) || null
  const concurrency: number = (options.concurrency as number) || CONCURRENCY_CONFIG.default
  
  console.log('ğŸš€ Parallel Test Generation (v2.4.0)\n')
  console.log(`ğŸ“Š Configuration:`)
  console.log(`   - Concurrency: ${concurrency}`)
  console.log(`   - Report: ${reportPath}`)
  if (priority) console.log(`   - Priority: ${priority}`)
  if (count) console.log(`   - Max functions: ${count}`)
  console.log()
  
  // 1. è¯»å– TODO å‡½æ•°
  console.log('ğŸ“‹ Reading TODO functions...')
  let todoFunctions = readTodoFunctions(reportPath, priority)
  
  if (count) {
    todoFunctions = todoFunctions.slice(0, count)
  }
  
  if (todoFunctions.length === 0) {
    console.log('âœ… No TODO functions found')
    return
  }
  
  console.log(`   Found ${todoFunctions.length} TODO functions\n`)
  
  // 2. åˆ†ç»„ä¸ºæ‰¹æ¬¡
  console.log('ğŸ“¦ Grouping into batches...')
  const batches = groupIntoBatches(todoFunctions, {
    minBatchSize: CONCURRENCY_CONFIG.minBatchSize,
    maxBatchSize: CONCURRENCY_CONFIG.maxBatchSize
  })
  
  console.log(`   Created ${batches.length} batches`)
  for (let i = 0; i < batches.length; i++) {
    console.log(`   - Batch ${i + 1}: ${batches[i].length} functions`)
  }
  console.log()
  
  // 3. åˆ›å»ºå·¥ä½œç›®å½•
  const workDir = 'reports/parallel_batches'
  mkdirSync(workDir, { recursive: true })
  
  // 4. å¹¶è¡Œç”Ÿæˆï¼ˆä½¿ç”¨ p-limit æ§åˆ¶å¹¶å‘ï¼‰
  console.log(`âš¡ Starting parallel generation (${concurrency} concurrent batches)...\n`)
  
  const limit = pLimit(concurrency)
  const startTime = Date.now()
  
  const results = await Promise.all(
    batches.map((batch, index) =>
      limit(() => generateBatch(batch, index, { reportPath, workDir }))
    )
  )
  
  const duration = ((Date.now() - startTime) / 1000).toFixed(2)
  
  // 5. æ±‡æ€»ç»“æœ
  console.log('\n\nğŸ“Š Generation Summary:')
  console.log('=' .repeat(60))
  
  const summary = {
    total: todoFunctions.length,
    success: 0,
    failed: 0,
    batches: results.length,
    duration,
    throughput: 0
  }
  
  const allSuccess = []
  const allFailed = []
  
  for (const result of results) {
    summary.success += result.success.length
    summary.failed += result.failed.length
    allSuccess.push(...result.success)
    allFailed.push(...result.failed)
  }
  
  summary.throughput = parseFloat((summary.total / parseFloat(duration)).toFixed(2))
  
  console.log(`Total functions: ${summary.total}`)
  console.log(`âœ… Success: ${summary.success}`)
  console.log(`âŒ Failed: ${summary.failed}`)
  console.log(`â±ï¸  Duration: ${summary.duration}s`)
  console.log(`ğŸ“ˆ Throughput: ${summary.throughput} functions/sec`)
  console.log('=' .repeat(60))
  
  // 6. æ›´æ–°æŠ¥å‘ŠçŠ¶æ€
  if (allSuccess.length > 0) {
    console.log(`\nâœï¸  Marking ${allSuccess.length} functions as DONE...`)
    updateFunctionStatus(reportPath, allSuccess, 'DONE')
  }
  
  if (allFailed.length > 0) {
    console.log(`âš ï¸  ${allFailed.length} functions remain as TODO (will retry next time)`)
  }
  
  // 7. ä¿å­˜è¯¦ç»†æŠ¥å‘Š
  const detailedReport = {
    timestamp: new Date().toISOString(),
    config: {
      concurrency,
      priority,
      count,
      batchCount: batches.length
    },
    summary,
    batches: results
  }
  
  const reportJson = join(workDir, 'parallel_report.json')
  writeFileSync(reportJson, JSON.stringify(detailedReport, null, 2), 'utf-8')
  console.log(`\nğŸ’¾ Detailed report saved: ${reportJson}`)
  
  console.log('\nğŸ‰ Parallel generation completed!')
}

/**
 * CLI å…¥å£ï¼ˆå¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶ï¼‰
 */
async function main(argv = process.argv) {
  const args = argv.slice(2)
  
  interface MainOptions {
    reportPath?: string
    priority?: string | null
    count?: number | null
    concurrency?: number
  }
  
  const options: MainOptions = {
    reportPath: 'reports/ut_scores.md',
    priority: null,
    count: null,
    concurrency: CONCURRENCY_CONFIG.default
  }
  
  // è§£æå‚æ•°
  for (let i = 0; i < args.length; i++) {
    if (args[i] === '-p' || args[i] === '--priority') {
      options.priority = args[++i] || null
    } else if (args[i] === '-n' || args[i] === '--count') {
      options.count = parseInt(args[++i] || '10')
    } else if (args[i] === '-c' || args[i] === '--concurrency') {
      options.concurrency = Math.min(parseInt(args[++i] || '3'), CONCURRENCY_CONFIG.maxConcurrency)
    } else if (args[i] === '--report') {
      options.reportPath = args[++i] || 'reports/ut_scores.md'
    }
  }
  
  try {
    await parallelGenerate(options)
  } catch (err: any) {
    const error = err as Error
    console.error('âŒ Parallel generation failed:', error?.message || String(err))
    process.exit(1)
  }
}

// å¦‚æœç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶
if (import.meta.url === `file://${process.argv[1]}`) {
  main().catch((err: any) => {
    const error = err as Error
    console.error('âŒ Fatal error:', error?.message || String(err))
    process.exit(1)
  })
}

